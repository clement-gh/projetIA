{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from grounding_dino_demo import detect_with_dino\n",
    "\n",
    "\n",
    "from const import *\n",
    "img_path = 'assets/61.jpg'\n",
    "text_prompt = \", \".join( ['cap', 't shirt', 'sunglasses', 'headband', 'shoe', 'sock', 'backpack', 'walking-sticks', 'bib numbers', 'trousers'])\n",
    "\n",
    "\n",
    "\n",
    "annotated_frame, boxes,phrases = detect_with_dino(img_path, text_prompt)\n",
    "cv2.imwrite('assets/61_dino.jpg', annotated_frame)\n",
    "print (phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from PIL import Image\n",
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(DEVICE), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cpu()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_WEIGHTS_PATH).to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "# Assurez-vous que les bibliothèques et les fonctions requises sont importées et définies avant d'exécuter ce code.\n",
    "\n",
    "\n",
    "def draw_masks_without_boxes(image, sam_model, boxes):\n",
    "    masks = segment(image, sam_model, boxes)\n",
    "    masks_without_boxes = np.zeros_like(image)  # Créer une image vide pour dessiner les masques sans les boîtes\n",
    "    \n",
    "    for mask in masks:\n",
    "        masks_without_boxes = draw_mask(mask, masks_without_boxes, random_color=True)  # Dessiner chaque masque\n",
    "    \n",
    "    return masks_without_boxes\n",
    "\n",
    "# Utilisation :\n",
    "# Remplacez les valeurs suivantes par vos propres valeurs\n",
    "\n",
    "\n",
    "# sam_model = Votre modèle\n",
    "# boxes = Vos boîtes\n",
    "\n",
    "# Appel de la fonction pour dessiner les masques sans les boîtes\n",
    "result_image = draw_masks_without_boxes(image, sam_predictor, boxes)\n",
    "\n",
    "# Afficher l'image résultante\n",
    "cv2.imwrite('assets/61_sam.jpg', result_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Redéfinition de la fonction draw_mask pour dessiner les masques directement sur l'image originale\n",
    "def draw_mask_on_image(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    \n",
    "    mask = mask.detach().cpu().numpy()  # Convertir le tenseur en tableau NumPy\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    mask_image_pil = Image.fromarray((mask_image * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "    image_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "\n",
    "    # Superposer le masque sur l'image originale\n",
    "    final_image = Image.alpha_composite(image_pil, mask_image_pil)\n",
    "\n",
    "    return np.array(final_image)\n",
    "\n",
    "def draw_masks_on_original_image(image, sam_model, boxes):\n",
    "    masks = segment(image, sam_model, boxes)\n",
    "    masks_list = []  # Liste pour stocker les masques générés\n",
    "    \n",
    "    for mask in masks:\n",
    "        image = draw_mask_on_image(mask, image, random_color=True)  # Dessiner chaque masque sur l'image originale\n",
    "        masks_list.append(mask.cpu().numpy())\n",
    "    return image, masks_list\n",
    "\n",
    "# Utilisation :\n",
    "# Remplacez les valeurs suivantes par vos propres valeurs\n",
    "# image = Votre image\n",
    "# sam_model = Votre modèle\n",
    "# boxes = Vos boîtes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Appel de la fonction pour dessiner les masques sur l'image originale\n",
    "result_image_with_masks, masks = draw_masks_on_original_image(image, sam_predictor, boxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Afficher l'image résultante\n",
    "cv2.imwrite('assets/61_sam_on_image.jpg', result_image_with_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator()\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "annotated_image = mask_annotator.annotate(scene=image.copy(),)\n",
    "#annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "\n",
    "sv.plot_image(annotated_image, (16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_masks(masks_list):\n",
    "    num_masks = len(masks_list)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_masks):\n",
    "        mask = masks_list[i][0] if len(masks_list[i].shape) == 3 else masks_list[i]  # Vérifier la forme du masque\n",
    "        plt.subplot(1, num_masks, i + 1)\n",
    "        plt.imshow(mask, cmap='gray')  # Afficher en niveaux de gris si le masque est 2D\n",
    "        plt.title(f\"Masque {i+1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Utilisation :\n",
    "# Remplacez masks_retrieved par votre liste de masques récupérés\n",
    "\n",
    "\n",
    "display_masks(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = result_image_with_masks.cpu().numpy()\n",
    "inverted_mask = ((1 - mask) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "image_source_pil = Image.fromarray(image)\n",
    "image_mask_pil = Image.fromarray(mask)\n",
    "inverted_image_mask_pil = Image.fromarray(inverted_mask)\n",
    "\n",
    "\n",
    "display(*[image_source_pil, image_mask_pil, inverted_image_mask_pil])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate, Model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from const import *\n",
    "import supervision as sv\n",
    "img_path = 'assets/61.jpg'\n",
    "text_prompt = ['cap', 'shirt', 'sunglasses', 'shoe', 'sock', 'backpack', 'sticks', 'bib', 'trousers']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_with_classes (image_path, text_prompt, box_threshold=0.3, text_threshold=0.3):\n",
    "    image = cv2.imread(image_path)\n",
    "    grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_T_CHECKPOINT_PATH, device=DEVICE)\n",
    "\n",
    "    detections, phrases = grounding_dino_model.predict_with_caption(\n",
    "        image=image,\n",
    "        caption=\", \".join(text_prompt),\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "    print (phrases)\n",
    "    classe_id_list_str =  grounding_dino_model.phrases2classes(phrases, text_prompt)\n",
    "    detections.class_id = classe_id_list_str\n",
    "    #detections = detections[detections.class_id != text_prompt.index(\"trousers\")]\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections, labels=phrases)\n",
    "\n",
    "    return annotated_image, detections, phrases\n",
    "\n",
    "annotated_frame, detections, phrases = detect_with_classes(img_path, text_prompt)\n",
    "print (detections)\n",
    "\n",
    "cv2.imwrite('assets/61_dino_classes.jpg', annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_WEIGHTS_PATH).to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "\n",
    "def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n",
    "    sam_predictor.set_image(image)\n",
    "    result_masks = []\n",
    "    for box in xyxy:\n",
    "        masks, scores, logits = sam_predictor.predict(\n",
    "            box=box,\n",
    "            multimask_output=True\n",
    "        )\n",
    "        index = np.argmax(scores)\n",
    "        result_masks.append(masks[index])\n",
    "    return np.array(result_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert detections to masks\n",
    "image = cv2.imread(img_path)\n",
    "detections.mask = segment(\n",
    "    sam_predictor=sam_predictor,\n",
    "    image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "    xyxy=detections.xyxy\n",
    ")\n",
    "\n",
    "# annotate image with detections\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "#annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections, labels=phrases)\n",
    "\n",
    "sv.plot_image(annotated_image, (16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate, Model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from const import *\n",
    "import supervision as sv\n",
    "img_path = 'assets/61.jpg'\n",
    "text_prompt = ['cap', 'shirt', 'sunglasses', 'shoe', 'sock', 'backpack', 'sticks', 'bib', 'trousers']\n",
    "\n",
    "from dino_detection import detect_with_dino\n",
    "\n",
    "annotated_frame, detections, phrases = detect_with_dino(img_path, text_prompt)\n",
    "print (detections)\n",
    "sv.plot_image( annotated_image, (16, 16))\n",
    "cv2.imwrite('assets/61_testdino.jpg', annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import supervision as sv\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from setup import *\n",
    "from const import *\n",
    "from dino_detection import detect_with_dino\n",
    "from segment import segment\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_available_weights())\n",
    "img_path = 'assets/DSC1167.jpg'\n",
    "text_prompt = ['person']\n",
    "img = cv2.imread(img_path)\n",
    "annotated_image, detections, phrases = detect_with_dino(img, text_prompt)\n",
    "\n",
    "segmented_image, detections2 = segment(detections, img)\n",
    "# enregistrer les images\n",
    "cv2.imwrite(\"assets/segmented_image.jpg\", segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (detections2)\n",
    "print (detections2.mask[1])\n",
    "# save the mask 1\n",
    "\n",
    "cv2.imwrite(\"assets/mask1.jpg\", detections2.mask[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# plot parameters\n",
    "grid_size_dimension = math.ceil(math.sqrt(len(detections.mask)))\n",
    "\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images = detections.mask,\n",
    "\n",
    "    grid_size = (grid_size_dimension, grid_size_dimension),\n",
    "    size = (16, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(detections.mask)):\n",
    "    # open the original mask\n",
    "    mask = detections.mask[i]\n",
    "    \n",
    "\n",
    "    # manual binarisation\n",
    "    for j in range(len(mask)):\n",
    "        for k in range(len(mask[j])):\n",
    "            if mask[j][k] :\n",
    "                mask[j][k] = 255\n",
    "            else:\n",
    "                mask[j][k] = 0\n",
    "\n",
    "    # save the new mask\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    cv2.imwrite(f\"assets/tests/mask{i}.jpg\", mask)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_list_of_masks(list_of_masks):\n",
    "    binarized_masks = list_of_masks\n",
    "    for i in range(len(list_of_masks)):\n",
    "        binarized_masks[i] = binarize_mask(list_of_masks[i])\n",
    "    return binarized_masks\n",
    "\n",
    "def binarize_mask(mask):\n",
    "    binarized_mask= mask\n",
    "    for j in range(len(mask)):\n",
    "        for k in range(len(mask[j])):\n",
    "            if mask[j][k] :\n",
    "                binarized_mask[j][k] = 255\n",
    "            else:\n",
    "                binarized_mask[j][k] = 0\n",
    "    binarized_mask = binarized_mask.astype(np.uint8) * 255\n",
    "    return binarized_mask\n",
    "\n",
    "def plot_masks(list_of_masks):\n",
    "    \n",
    "    for i in range(len(list_of_masks)):\n",
    "        # get mask information\n",
    "        mask = list_of_masks[i]\n",
    "        \n",
    "\n",
    "        # show the mask\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(mask, cmap='gray')\n",
    "        ax.set_axis_off()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masks(binarize_list_of_masks(detections.mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (binarize_mask(detections.mask[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def colorize_mask(bin_mask, img):\n",
    "    \n",
    "    img_arr = np.asarray(img)\n",
    "    bin_mask = np.expand_dims(bin_mask, axis=2)\n",
    "    result_arr = np.bitwise_and(img_arr, bin_mask)\n",
    "    result_img = Image.fromarray(result_arr)\n",
    "    #convert to RGB with cv2\n",
    "    result_img = cv2.cvtColor(np.array(result_img), cv2.COLOR_BGR2RGB)\n",
    "    return result_img\n",
    "def colorize_list_of_masks(list_of_masks, img):\n",
    "    colorized_masks = []\n",
    "    for i in range(len(list_of_masks)):\n",
    "        colorized_masks.append(colorize_mask(list_of_masks[i], img))\n",
    "    return colorized_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(colorize_mask(binarize_mask(detections.mask[0]), img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from process import *\n",
    "from color_detection import *\n",
    "from color_classifier   import *\n",
    "from yolo_detection import detect_objects, concat_bib_numbers, extract_bib_numbers,sort_bib_numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'assets/DSC1167.jpg'\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "listA = first_step(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_step(listA, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step3(img_path):\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    text_prompt = ['cap', 'shirt', 'sunglasses', 'shoe', 'sock', 'backpack', 'sticks', 'bib', 'trousers']\n",
    "    annotated_image, segmented_image, detections, phrases = detect_and_segment(img, text_prompt)\n",
    "\n",
    "    \n",
    "    colorized_masks =colorize_list_of_masks(binarize_list_of_masks(detections.mask), img)\n",
    "    filtered_masks = []\n",
    "    filtered_phrases = []\n",
    "    for i in range(len(detections.mask)):\n",
    "        if phrases not in ['bib', 'sticks']:\n",
    "            filtered_masks.append(colorized_masks[i])\n",
    "            filtered_phrases.append(phrases[i])\n",
    "    #tab_names , average_colors_hexa=determine_color_v2(colorized_masks,phrases)\n",
    "    tab_names , average_colors_hexa=determine_color_v2(filtered_masks,filtered_phrases)\n",
    "    return tab_names , average_colors_hexa,detections, phrases,colorized_masks,annotated_image, segmented_image\n",
    "\n",
    "\n",
    "\n",
    "# verification des doublons de detection\n",
    "\"\"\"\n",
    "for i in range(len(detections)):\n",
    "    for j in range(len(detections)):\n",
    "        if i != j:\n",
    "            if detections[i].class_id == detections[j].class_id:\n",
    "                LOGGER.warning(f\"double detection for {detections[i].class_id} at {i} and {j}\")\n",
    "                # supprimer la detection le de plus petit score\n",
    "                if detections[i].score < detections[j].score:\n",
    "                    detections.remove(detections[i])\n",
    "                else:\n",
    "                    detections.remove(detections[j])\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from process import *\n",
    "from color_detection import *\n",
    "from color_classifier   import *\n",
    "from yolo_detection import detect_objects, concat_bib_numbers, extract_bib_numbers,sort_bib_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shirt\n",
      "cap\n",
      "trousers\n",
      "Temps d'exécution : 2.575484275817871 secondes\n",
      "(223, 0.74, 0.66)\n",
      "shirt\n",
      "Temps d'exécution : 1.5374042987823486 secondes\n",
      "(353, 0.06, 0.73)\n",
      "cap\n",
      "Temps d'exécution : 2.815484046936035 secondes\n",
      "(229, 0.34, 0.29)\n",
      "trousers\n",
      "step3 done\n",
      "shirt :  Blue (223, 0.74, 0.66)\n",
      "cap :  Grey (353, 0.06, 0.73)\n",
      "trousers :  Grey (229, 0.34, 0.29)\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      "{'person': None, 'cap': {'detected': False, 'color': None}, 'shirt': {'detected': False, 'color': None}, 'sunglasses': {'detected': False}, 'shoe': {'detected': False}, 'sock': {'detected': False}, 'backpack': {'detected': False}, 'sticks': {'detected': False}, 'number': {'detected': False, 'numbers': None}, 'trousers': {'detected': False, 'color': None}}\n",
      "Blue\n",
      "Grey\n",
      "Grey\n",
      "{'person': 'gg', 'cap': {'detected': True, 'color': 'Grey'}, 'shirt': {'detected': True, 'color': 'Blue'}, 'sunglasses': {'detected': False}, 'shoe': {'detected': True}, 'sock': {'detected': True}, 'backpack': {'detected': False}, 'sticks': {'detected': False}, 'number': {'detected': True, 'numbers': '3202'}, 'trousers': {'detected': True, 'color': 'Grey'}}\n"
     ]
    }
   ],
   "source": [
    "img_p =  'assets/tests/p_2_259399375.jpg'\n",
    "img = cv2.imread(img_p)\n",
    "tab_names , average_colors_hexa,detections, phrases, annotated_image,segmented_image,croped_bib , text_prompt= step3(img)\n",
    "cv2.imwrite('assets/tests/segmented_image.jpg', segmented_image)\n",
    "cv2.imwrite('assets/tests/annotated_image.jpg', annotated_image)\n",
    "dict = step5(tab_names , average_colors_hexa, phrases ,croped_bib, text_prompt, \"gg\")\n",
    "print (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (average_colors_hexa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step42(tab_names , average_colors_hexa,detections, phrases ,colorized_masks):\n",
    "    # recuperer le mask du bib \n",
    "    # si on detecte le bib\n",
    "    if 'number' in phrases:\n",
    "        for  mask , label in zip (colorized_masks , phrases):\n",
    "            if label == 'number':\n",
    "                mask_bib = mask\n",
    "                croped_bib = crop_bip_numbers(phrases, mask_bib, detections)\n",
    "    else:\n",
    "        croped_bib = None\n",
    "    return croped_bib\n",
    "\n",
    "def step52 (tab_names , average_colors_hexa,detections, phrases ,colorized_masks,croped_bib):\n",
    "    dict_color = {}\n",
    "    for name , color in zip (tab_names , average_colors_hexa ):\n",
    "        detected_color = color_or_grayscale(color)\n",
    "        print(name ,\": \", color_or_grayscale(color),color)\n",
    "        dict_color[name] = detected_color\n",
    "    if croped_bib is not None:\n",
    "        detected_objects = detect_objects(croped_bib, device=DEVICE)\n",
    "        bibs= concat_bib_numbers(extract_bib_numbers(sort_bib_numbers(detected_objects)))\n",
    "        print (bibs)\n",
    "        # parcourir les phrases et le dictionnaire\n",
    "        for phrase in phrases:\n",
    "            if phrase in dict:\n",
    "                print (\"ok\")\n",
    "                if phrase in ['cap', 'shirt', 'trousers']:\n",
    "                    print (dict_color[phrase])\n",
    "                    dict[phrase]['color'] = dict_color[phrase]\n",
    "                    dict[phrase]['detected'] = True\n",
    "                elif phrase == 'number':\n",
    "                    dict[phrase]['detected'] = True\n",
    "                    dict[phrase]['numbers'] = bibs\n",
    "                else:\n",
    "                    dict[phrase]['detected'] = True\n",
    "        \n",
    "    return dict\n",
    "\n",
    "crop_bip=step42(tab_names , average_colors_hexa,detections, phrases,colorized_masks)\n",
    "\n",
    "print (step52(tab_names , average_colors_hexa,detections, phrases ,colorized_masks,crop_bip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step5 (tab_names , average_colors_hexa,detections, phrases ,colorized_masks,croped_bib):\n",
    "    dict_color = {}\n",
    "    for name , color in zip (tab_names , average_colors_hexa ):\n",
    "        detected_color = color_or_grayscale(color)\n",
    "    #print(name ,\": \", color_or_grayscale(color),color)\n",
    "        dict_color[name] = detected_color\n",
    "    # si on detecte le bib\n",
    "\n",
    "    print (dict_color)\n",
    "    if croped_bib is not None:\n",
    "        detected_objects = detect_objects(croped_bib, device=DEVICE)\n",
    "        bibs= concat_bib_numbers(extract_bib_numbers(sort_bib_numbers(detected_objects)))\n",
    "    for detection in dict_color :\n",
    "        if detection in dict:\n",
    "            dict[detection]['detected'] = True\n",
    "            if detection != 'bib':\n",
    "                dict[detection]['color'] = dict_color[detection]\n",
    "            else:\n",
    "                dict[detection]['numbers'] = bibs\n",
    "        else:\n",
    "            dict[detection]['detected'] = False\n",
    "    return dict\n",
    "print (step5(tab_names , average_colors_hexa,detections, phrases ,colorized_masks,crop_bip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv2.imwrite('assets/tests/p_0_259399375_dino.jpg', annotated_image)\n",
    "cv2.imwrite('assets/tests/p_0_259399375_segmented.jpg', segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: supprimer affichage des couleurs\n",
    "\n",
    "print (dict_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bib_numbers_index(tab_labels):\n",
    "    index = 0\n",
    "    for label in tab_labels:\n",
    "        if label == 'bib':\n",
    "            return index\n",
    "        index += 1\n",
    "    return None\n",
    "\n",
    "def crop_bip_numbers(tab_labels,image):\n",
    "    index= get_bib_numbers_index(tab_labels)\n",
    "    if index is not None:\n",
    "        # rounding the float numbers\n",
    "        x1 = math.floor(detections.xyxy[index][0])\n",
    "        y1 = math.floor(detections.xyxy[index][1])\n",
    "        x2 = math.floor(detections.xyxy[index][2])\n",
    "        y2 = math.floor(detections.xyxy[index][3])\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        # crop the original image with the detection box\n",
    "        cropped_image = image.crop((x1, y1, x2, y2))\n",
    "        return cropped_image\n",
    "    else:\n",
    "        print(\"any bip numbers found\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "croped_bib = crop_bip_numbers(phrases, img)\n",
    "\n",
    "if croped_bib is not None:\n",
    "    croped_bib.save(\"assets/tests/bib.jpg\")\n",
    "    print(\"bib numbers saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Spécifiez le chemin complet de votre répertoire YOLOv7\n",
    "yolov7_path = Path('./yolov7')\n",
    "sys.path.append(str(yolov7_path.resolve()))\n",
    "from yolov7.detect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "source_path = \"./assets/tests/bib.jpg\" # Remplacez cela par votre chemin d'accès à l'image ou à la vidéo\n",
    "img = cv2.imread(source_path)\n",
    "detected_objects = detect_objects(img)\n",
    "\n",
    "\n",
    "for detection in detected_objects:\n",
    "    print(f\"Class: {detection['class']}, Label: {detection['label']}, Confidence: {detection['confidence']}, Box: {detection['box']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# afficher les numéros de dossard\n",
    "print(concat_bib_numbers(extract_bib_numbers(sort_bib_numbers(detected_objects))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplissage du dictionnaire\n",
    "# pour chaque objet  on verifie si il est detecté\n",
    "# si oui on rempli la couleur ou le numero de dossard\n",
    "for detection in dict_color :\n",
    "    if detection in dict:\n",
    "        dict[detection]['detected'] = True\n",
    "        if detection != 'bib':\n",
    "            dict[detection]['color'] = dict_color[detection]\n",
    "        else:\n",
    "            dict[detection]['numbers'] = concat_bib_numbers(extract_bib_numbers(sort_bib_numbers(detected_objects)))\n",
    "    else:\n",
    "        dict[detection]['detected'] = False\n",
    "\n",
    "print (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = ['person']\n",
    "annotated_image, segmented_image, detections, phrases = detect_and_segment(img, text_prompt)\n",
    "\n",
    "binarized_list_of_masks=binarize_list_of_masks(detections.mask)\n",
    "colorized_list_of_masks=colorize_list_of_masks(binarized_list_of_masks, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( binarized_list_of_masks[0])\n",
    "print (binarize_mask(detections.mask[0]))\n",
    "print (binarized_list_of_masks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_step(colorized_list_of_masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
